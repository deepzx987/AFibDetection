{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, mkdir, system\n",
    "from os.path import isfile, isdir, join, exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv1D, BatchNormalization, Dropout, MaxPooling1D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "import itertools\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "fs = 300\n",
    "input_dir = 'One_Hot_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, name, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(name+'confmat.png',dpi=250)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(61836, 1200, 1)\n",
      "(61836, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1200, 64)          11584     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1200, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1200, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 600, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4915328   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 4,927,555\n",
      "Trainable params: 4,927,427\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 50086 samples, validate on 5566 samples\n",
      "Epoch 1/1\n",
      "50086/50086 [==============================] - 229s 5ms/step - loss: 3.8896 - acc: 0.4214 - val_loss: 4.8222 - val_acc: 0.3953\n",
      "TP,FP,TN,FN,Precision,Accuracy,Sensitivity,Specificity,F1Score,AUC\n",
      "203 , 173 , 2752 , 3056 , 53.99% , 47.78% , 6.23% , 94.09% , 11.17% , 50.16%\n",
      "1423 , 2397 , 1694 , 670 , 37.25% , 50.40% , 67.99% , 41.41% , 48.13% , 54.70%\n",
      "757 , 1231 , 4121 , 75 , 38.08% , 78.88% , 90.99% , 77.00% , 53.69% , 83.99%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.05      0.07      1219\n",
      "           1       0.37      0.68      0.48      2093\n",
      "           2       0.38      0.91      0.54       832\n",
      "\n",
      "   micro avg       0.36      0.54      0.43      4144\n",
      "   macro avg       0.30      0.55      0.36      4144\n",
      "weighted avg       0.31      0.54      0.37      4144\n",
      " samples avg       0.36      0.36      0.36      4144\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepankar/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(41222, 1800, 1)\n",
      "(41222, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 1800, 64)          11584     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1800, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1800, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 900, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,385,155\n",
      "Trainable params: 7,385,027\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 33389 samples, validate on 3710 samples\n",
      "Epoch 1/1\n",
      "33389/33389 [==============================] - 225s 7ms/step - loss: 3.9371 - acc: 0.4181 - val_loss: 4.9379 - val_acc: 0.3795\n",
      "TP,FP,TN,FN,Precision,Accuracy,Sensitivity,Specificity,F1Score,AUC\n",
      "50 , 35 , 1950 , 2088 , 58.82% , 48.51% , 2.34% , 98.24% , 4.50% , 50.29%\n",
      "1034 , 1623 , 1066 , 400 , 38.92% , 50.93% , 72.11% , 39.64% , 50.55% , 55.87%\n",
      "503 , 878 , 2694 , 48 , 36.42% , 77.54% , 91.29% , 75.42% , 52.07% , 83.35%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.02      0.03       766\n",
      "           1       0.39      0.72      0.51      1434\n",
      "           2       0.36      0.91      0.52       551\n",
      "\n",
      "   micro avg       0.38      0.56      0.45      2751\n",
      "   macro avg       0.30      0.55      0.35      2751\n",
      "weighted avg       0.32      0.56      0.38      2751\n",
      " samples avg       0.38      0.38      0.38      2751\n",
      "\n",
      "2\n",
      "(123676, 600, 1)\n",
      "(123676, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 600, 64)           11584     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 600, 64)           256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 600, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 300, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               2457728   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,469,955\n",
      "Trainable params: 2,469,827\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 100177 samples, validate on 11131 samples\n",
      "Epoch 1/1\n",
      "100177/100177 [==============================] - 225s 2ms/step - loss: 2.2666 - acc: 0.4625 - val_loss: 1.1468 - val_acc: 0.4125\n",
      "TP,FP,TN,FN,Precision,Accuracy,Sensitivity,Specificity,F1Score,AUC\n",
      "1788 , 1060 , 4878 , 4642 , 62.78% , 53.90% , 27.81% , 82.15% , 38.54% , 54.98%\n",
      "2129 , 3165 , 5044 , 2030 , 40.22% , 58.00% , 51.19% , 61.44% , 45.04% , 56.32%\n",
      "1549 , 2677 , 7912 , 230 , 36.65% , 76.50% , 87.07% , 74.72% , 51.59% , 80.90%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.37      0.33      2377\n",
      "           1       0.40      0.51      0.45      4159\n",
      "           2       0.37      0.87      0.52      1779\n",
      "\n",
      "   micro avg       0.37      0.55      0.44      8315\n",
      "   macro avg       0.36      0.58      0.43      8315\n",
      "weighted avg       0.37      0.55      0.43      8315\n",
      " samples avg       0.37      0.37      0.37      8315\n",
      "\n",
      "7\n",
      "(35331, 2100, 1)\n",
      "(35331, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 2100, 64)          11584     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2100, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2100, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1050, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 67200)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8601728   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 8,613,955\n",
      "Trainable params: 8,613,827\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Train on 28617 samples, validate on 3180 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "for f in listdir(input_dir):\n",
    "    print (f[0])\n",
    "    df = pd.read_csv(join(input_dir,f), header=None)\n",
    "    data = df.values\n",
    "    X = data[:,:-4]\n",
    "    X.shape\n",
    "    X = X.reshape(-1, X.shape[1], 1)\n",
    "    y = data[:,-3:]\n",
    "    print (X.shape)\n",
    "    print (y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1, shuffle=True)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=(180), activation='relu', input_shape=(X.shape[1],1), \n",
    "                     padding='same', strides=1, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(filters=16, kernel_size=(90), activation='relu', \n",
    "                     padding='same', strides=1, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Conv1D(filters=4, kernel_size=(45), activation='relu', \n",
    "                     padding='same', strides=1, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(X_train, y_train, batch_size=1000, epochs=1, verbose=1, validation_data=(X_val, y_val), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, verbose=1)])\n",
    "    \n",
    "    accuracy = history.history['acc']\n",
    "    val_accuracy = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(accuracy))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(epochs, accuracy, label='Training accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f[0]+'acc.png',dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(epochs, loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f[0]+'loss.png',dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    model.save(f[0]+\"model.h5py\")\n",
    "\n",
    "    new_y_pred = model.predict(X_test)\n",
    "    y_test_arg = np.argmax(np.round(y_test),axis=1)\n",
    "    y_pred_arg = np.argmax(np.round(new_y_pred),axis=1)\n",
    "    y_pred = np.zeros((new_y_pred.shape[0],new_y_pred.shape[1]))\n",
    "\n",
    "    from pycm import *\n",
    "    cm = ConfusionMatrix(actual_vector=y_test_arg, predict_vector=y_pred_arg)\n",
    "    totalt = cm.__dict__\n",
    "\n",
    "    TP = totalt['TP']\n",
    "    FP = totalt['FP']\n",
    "    TN = totalt['TN']\n",
    "    FN = totalt['FN']\n",
    "\n",
    "    PPV = totalt['PPV']\n",
    "    ACC = totalt['ACC']\n",
    "    SEN = totalt['TPR']\n",
    "    SPE = totalt['TNR']\n",
    "    F1S = totalt['F1']\n",
    "    AUC = totalt['AUC']\n",
    "\n",
    "    print ('TP,FP,TN,FN,Precision,Accuracy,Sensitivity,Specificity,F1Score,AUC')\n",
    "    print (TP[0], ',', FP[0], ',', TN[0], ',', FN[0], ',', '{:.2f}%'.format(PPV[0]*100), ',', '{:.2f}%'.format(ACC[0]*100), ',', '{:.2f}%'.format(SEN[0]*100), ',','{:.2f}%'.format(SPE[0]*100), ',','{:.2f}%'.format(F1S[0]*100), ',','{:.2f}%'.format(AUC[0]*100))\n",
    "    print (TP[1], ',', FP[1], ',', TN[1], ',', FN[1], ',', '{:.2f}%'.format(PPV[1]*100), ',','{:.2f}%'.format(ACC[1]*100), ',','{:.2f}%'.format(SEN[1]*100), ',','{:.2f}%'.format(SPE[1]*100), ',','{:.2f}%'.format(F1S[1]*100), ',','{:.2f}%'.format(AUC[1]*100))\n",
    "    print (TP[2], ',', FP[2], ',', TN[2], ',', FN[2], ',', '{:.2f}%'.format(PPV[2]*100), ',','{:.2f}%'.format(ACC[2]*100), ',','{:.2f}%'.format(SEN[2]*100), ',','{:.2f}%'.format(SPE[2]*100), ',','{:.2f}%'.format(F1S[2]*100), ',','{:.2f}%'.format(AUC[2]*100))\n",
    "\n",
    "\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        y_pred[i][y_pred_arg[i]] = 1\n",
    "\n",
    "    print (classification_report(y_test, y_pred))\n",
    "    cnf_matrix = confusion_matrix(y_test_arg, y_pred_arg)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    class_names = ['Normal','AFiB','Other', 'Noisy']\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plot_confusion_matrix(cnf_matrix, name = f[0], classes=class_names, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "\n",
    "# test_eval = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print('Test loss:', test_eval[0])\n",
    "# print('Test accuracy:', test_eval[1])\n",
    "\n",
    "# accuracy = history.history['acc']\n",
    "# val_accuracy = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(len(accuracy))\n",
    "# plt.figure(figsize=(20,5))\n",
    "# plt.plot(epochs, accuracy, label='Training accuracy')\n",
    "# plt.plot(epochs, val_accuracy, label='Validation accuracy')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.savefig('acc.png',dpi=250)\n",
    "# plt.close()\n",
    "# plt.figure(figsize=(20,5))\n",
    "# plt.plot(epochs, loss, label='Training loss')\n",
    "# plt.plot(epochs, val_loss, label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "# plt.savefig('loss.png',dpi=250)\n",
    "# plt.close()\n",
    "\n",
    "# 81% recall. That means the model correctly identified 81% of the total bad loans. That’s pretty great. \n",
    "# But is this actually representative of how the model will perform? To find out, I’ll calculate the accuracy \n",
    "# and recall for the model on the test dataset I created initially.\n",
    "# By oversampling before splitting into training and validation datasets, I “bleed” information from the\n",
    "# validation set into the training of the model.\n",
    "\n",
    "# To see how this works, think about the case of simple oversampling (where I just duplicate observations). \n",
    "# If I upsample a dataset before splitting it into a train and validation set, I could end up with the same \n",
    "# observation in both datasets. As a result, a complex enough model will be able to perfectly predict the value \n",
    "# for those observations when predicting on the validation set, inflating the accuracy and recall.\n",
    "\n",
    "# When upsampling using SMOTE, I don’t create duplicate observations. However, because the SMOTE algorithm uses\n",
    "# the nearest neighbors of observations to create synthetic data, it still bleeds information. If the nearest \n",
    "# neighbors of minority class observations in the training set end up in the validation set, their information \n",
    "# is partially captured by the synthetic data in the training set. Since I’m splitting the data randomly, we’d\n",
    "# expect to have this happen. As a result, the model will be better able to predict validation set values than\n",
    "# completely new data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
